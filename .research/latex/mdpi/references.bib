@article{aguilarsaavedra-2024-reprompting,
 author = {J. A. Aguilar-Saavedra},
 title = {Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling},
 year = {2024}
}

@article{an-2024-improving,
 author = {Haipeng An and Shuailiang Ge and Jia Liu and Mingzhe Liu},
 doi = {10.1103/PhysRevLett.134.171001},
 title = {Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning},
 year = {2024}
}

@article{arves-2024-position,
 author = {J. Arvesú and A. M. Ramírez-Aberasturis},
 doi = {10.1080/10652469.2020.1830990},
 title = {Position: LLMs Need a Bayesian Meta-Reasoning Framework for More Robust and Generalizable Reasoning},
 year = {2024}
}

@article{banerjee-2024-coke,
 author = {Arka Banerjee},
 doi = {10.2140/agt.2025.25.4391},
 title = {COKE: A Cognitive Knowledge Graph for Machine Theory of Mind},
 year = {2024}
}

@article{borges-2023-mind,
 author = {Yulle G. F. Borges and Rafael C. S. Schouery and Flávio K. Miyazawa},
 title = {Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse},
 year = {2023}
}

@article{dziri-2023-leap,
 author = {Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jiang and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},
 title = {Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge},
 year = {2023}
}

@article{ekstedt-2024-comparing,
 author = {Andreas Ekstedt and Philipp Schicho and Tuomas V. I. Tenkanen},
 title = {Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning},
 year = {2024}
}

@article{goren-2024-controlling,
 author = {Matan Goren and Eran Treister},
 title = {Controlling Thinking Speed in Reasoning Models},
 year = {2024}
}

@article{kojima-2022-large,
 author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
 title = {Large Language Models are Zero-Shot Reasoners},
 year = {2022}
}

@article{lavanakul-2024-transformers,
 author = {Will Lavanakul and Jason J. Choi and Koushil Sreenath and Claire J. Tomlin},
 title = {Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought},
 year = {2024}
}

@article{maltese-2023-automatic,
 author = {David Maltese and Chokri Ogabi},
 title = {Automatic Chain of Thought Prompting in Large Language Models},
 year = {2023}
}

@article{takeda-2023-think,
 author = {Yoichi Takeda},
 title = {Think about it! Improving defeasible reasoning by first modeling the question scenario.},
 year = {2023}
}

@article{tautges-2023-from,
 author = {James Tautges},
 title = {From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning},
 year = {2023}
}

@article{wei-2022-chain,
 author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 year = {2022}
}